\chapter{Tests et Validation}

\section{Stratégie de Tests}

\subsection{Approche Test-Driven Development}

Notre approche de tests suit la pyramide de tests classique avec une emphase particulière sur l'automatisation et la couverture de code.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/testing_pyramid.png}
    \caption{Pyramide des tests AWS Next Express}
    \label{fig:testing_pyramid}
\end{figure}

\subsection{Types de Tests Implémentés}

\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|l|X|X|c|}
        \hline
        \textbf{Type de Test} & \textbf{Objectif} & \textbf{Outils} & \textbf{Couverture} \\
        \hline
        Tests Unitaires & Validation des fonctions isolées & Jest, React Testing Library & 85\% \\
        \hline
        Tests d'Intégration & Validation des API et bases de données & Supertest, Jest & 70\% \\
        \hline
        Tests de Composants & Validation des composants React & Testing Library & 80\% \\
        \hline
        Tests End-to-End & Validation des workflows complets & Playwright & 60\% \\
        \hline
        Tests de Performance & Validation des performances & Lighthouse CI & 90\% \\
        \hline
    \end{tabularx}
    \caption{Types de tests et couverture}
    \label{tab:test_types}
\end{table}

\section{Tests Unitaires}

\subsection{Configuration Jest}

\begin{lstlisting}[language=TypeScript, caption=Configuration Jest]
import type { Config } from 'jest';
import nextJest from 'next/jest';

const createJestConfig = nextJest({
  dir: './',
});

const config: Config = {
  coverageProvider: 'v8',
  testEnvironment: 'jsdom',
  setupFilesAfterEnv: ['<rootDir>/jest.setup.js'],
  moduleNameMapper: {
    '^@/(.*)$': '<rootDir>/$1',
    '^@/components/(.*)$': '<rootDir>/components/$1',
    '^@/lib/(.*)$': '<rootDir>/lib/$1',
  },
  collectCoverageFrom: [
    'components/**/*.{ts,tsx}',
    'lib/**/*.{ts,tsx}',
    'app/**/*.{ts,tsx}',
    '!**/*.d.ts',
    '!**/node_modules/**',
  ],
  coverageThreshold: {
    global: {
      branches: 80,
      functions: 80,
      lines: 80,
      statements: 80,
    },
  },
};

export default createJestConfig(config);
\end{lstlisting}

\subsection{Tests des Utilitaires DynamoDB}

\begin{lstlisting}[language=TypeScript, caption=Tests DynamoDB]
import { createUser, getAllUsers, getUserById, updateUser, deleteUser } from '@/lib/db/users-dynamodb';
import { mockDynamoDBDocumentClient } from '@aws-sdk/lib-dynamodb';

jest.mock('@aws-sdk/lib-dynamodb');

describe('DynamoDB User Operations', () => {
  const mockSend = jest.fn();
  
  beforeEach(() => {
    mockDynamoDBDocumentClient.prototype.send = mockSend;
    mockSend.mockClear();
  });

  describe('createUser', () => {
    it('should create a user successfully', async () => {
      const userData = {
        name: 'John Doe',
        email: 'john@example.com',
        profilePictureUrl: 'https://example.com/profile.jpg',
      };

      mockSend.mockResolvedValueOnce({});

      const result = await createUser(userData);

      expect(result).toMatchObject({
        name: 'John Doe',
        email: 'john@example.com',
        profilePictureUrl: 'https://example.com/profile.jpg',
      });
      expect(result.id).toBeDefined();
      expect(result.createdAt).toBeDefined();
      expect(result.updatedAt).toBeDefined();
    });

    it('should handle DynamoDB errors', async () => {
      const userData = {
        name: 'John Doe',
        email: 'john@example.com',
        profilePictureUrl: 'https://example.com/profile.jpg',
      };

      mockSend.mockRejectedValueOnce(new Error('DynamoDB Error'));

      await expect(createUser(userData)).rejects.toThrow('DynamoDB Error');
    });
  });

  describe('getUserById', () => {
    it('should return user when found', async () => {
      const mockUser = {
        id: 'test-id',
        name: 'John Doe',
        email: 'john@example.com',
        profilePictureUrl: 'https://example.com/profile.jpg',
        createdAt: '2023-01-01T00:00:00.000Z',
        updatedAt: '2023-01-01T00:00:00.000Z',
      };

      mockSend.mockResolvedValueOnce({ Item: mockUser });

      const result = await getUserById('test-id');

      expect(result).toEqual(mockUser);
    });

    it('should return null when user not found', async () => {
      mockSend.mockResolvedValueOnce({});

      const result = await getUserById('non-existent-id');

      expect(result).toBeNull();
    });
  });
});
\end{lstlisting}

\section{Tests de Composants}

\subsection{Tests des Composants UI}

\begin{lstlisting}[language=TypeScript, caption=Tests de composants React]
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import userEvent from '@testing-library/user-event';
import { UserForm } from '@/components/user-form';

// Mock Next.js router
jest.mock('next/navigation', () => ({
  useRouter: () => ({
    push: jest.fn(),
    refresh: jest.fn(),
  }),
}));

describe('UserForm Component', () => {
  const mockOnSubmit = jest.fn();

  beforeEach(() => {
    mockOnSubmit.mockClear();
  });

  it('renders form fields correctly', () => {
    render(<UserForm onSubmit={mockOnSubmit} />);

    expect(screen.getByLabelText(/name/i)).toBeInTheDocument();
    expect(screen.getByLabelText(/email/i)).toBeInTheDocument();
    expect(screen.getByLabelText(/profile picture/i)).toBeInTheDocument();
    expect(screen.getByRole('button', { name: /create user/i })).toBeInTheDocument();
  });

  it('validates required fields', async () => {
    const user = userEvent.setup();
    render(<UserForm onSubmit={mockOnSubmit} />);

    const submitButton = screen.getByRole('button', { name: /create user/i });
    await user.click(submitButton);

    await waitFor(() => {
      expect(screen.getByText(/name is required/i)).toBeInTheDocument();
      expect(screen.getByText(/email is required/i)).toBeInTheDocument();
    });

    expect(mockOnSubmit).not.toHaveBeenCalled();
  });

  it('validates email format', async () => {
    const user = userEvent.setup();
    render(<UserForm onSubmit={mockOnSubmit} />);

    const emailInput = screen.getByLabelText(/email/i);
    await user.type(emailInput, 'invalid-email');

    const submitButton = screen.getByRole('button', { name: /create user/i });
    await user.click(submitButton);

    await waitFor(() => {
      expect(screen.getByText(/please enter a valid email/i)).toBeInTheDocument();
    });
  });

  it('submits form with valid data', async () => {
    const user = userEvent.setup();
    const file = new File(['test'], 'test.jpg', { type: 'image/jpeg' });
    
    render(<UserForm onSubmit={mockOnSubmit} />);

    await user.type(screen.getByLabelText(/name/i), 'John Doe');
    await user.type(screen.getByLabelText(/email/i), 'john@example.com');
    
    const fileInput = screen.getByLabelText(/profile picture/i);
    await user.upload(fileInput, file);

    const submitButton = screen.getByRole('button', { name: /create user/i });
    await user.click(submitButton);

    await waitFor(() => {
      expect(mockOnSubmit).toHaveBeenCalledWith({
        name: 'John Doe',
        email: 'john@example.com',
        profilePicture: file,
      });
    });
  });

  it('shows loading state during submission', async () => {
    const user = userEvent.setup();
    const slowMockOnSubmit = jest.fn(() => new Promise(resolve => setTimeout(resolve, 1000)));
    
    render(<UserForm onSubmit={slowMockOnSubmit} />);

    await user.type(screen.getByLabelText(/name/i), 'John Doe');
    await user.type(screen.getByLabelText(/email/i), 'john@example.com');

    const submitButton = screen.getByRole('button', { name: /create user/i });
    await user.click(submitButton);

    expect(screen.getByText(/creating user/i)).toBeInTheDocument();
    expect(submitButton).toBeDisabled();
  });
});
\end{lstlisting}

\section{Tests d'Intégration}

\subsection{Tests des API Routes}

\begin{lstlisting}[language=TypeScript, caption=Tests d'intégration API]
import { createMocks } from 'node-mocks-http';
import { GET, POST } from '@/app/api/users/route';

// Mock DynamoDB
jest.mock('@/lib/db/users-dynamodb');

describe('/api/users API Route', () => {
  describe('GET /api/users', () => {
    it('should return all users', async () => {
      const mockUsers = [
        {
          id: '1',
          name: 'John Doe',
          email: 'john@example.com',
          profilePictureUrl: 'https://example.com/profile1.jpg',
          createdAt: '2023-01-01T00:00:00.000Z',
          updatedAt: '2023-01-01T00:00:00.000Z',
        },
        {
          id: '2',
          name: 'Jane Smith',
          email: 'jane@example.com',
          profilePictureUrl: 'https://example.com/profile2.jpg',
          createdAt: '2023-01-02T00:00:00.000Z',
          updatedAt: '2023-01-02T00:00:00.000Z',
        },
      ];

      const { getAllUsers } = require('@/lib/db/users-dynamodb');
      getAllUsers.mockResolvedValue(mockUsers);

      const { req } = createMocks({ method: 'GET' });
      const response = await GET(req);
      const data = await response.json();

      expect(response.status).toBe(200);
      expect(data).toEqual(mockUsers);
    });

    it('should handle database errors', async () => {
      const { getAllUsers } = require('@/lib/db/users-dynamodb');
      getAllUsers.mockRejectedValue(new Error('Database connection failed'));

      const { req } = createMocks({ method: 'GET' });
      const response = await GET(req);

      expect(response.status).toBe(500);
    });
  });

  describe('POST /api/users', () => {
    it('should create a new user', async () => {
      const mockUser = {
        id: 'new-user-id',
        name: 'New User',
        email: 'newuser@example.com',
        profilePictureUrl: 'https://example.com/new-profile.jpg',
        createdAt: '2023-01-03T00:00:00.000Z',
        updatedAt: '2023-01-03T00:00:00.000Z',
      };

      const { createUser } = require('@/lib/db/users-dynamodb');
      createUser.mockResolvedValue(mockUser);

      // Mock S3 upload
      const { uploadToS3 } = require('@/lib/aws/s3');
      uploadToS3.mockResolvedValue('https://example.com/new-profile.jpg');

      const formData = new FormData();
      formData.append('name', 'New User');
      formData.append('email', 'newuser@example.com');
      formData.append('profilePicture', new File(['test'], 'test.jpg', { type: 'image/jpeg' }));

      const { req } = createMocks({
        method: 'POST',
        headers: { 'content-type': 'multipart/form-data' },
      });

      // Mock request.formData()
      req.formData = jest.fn().mockResolvedValue(formData);

      const response = await POST(req);
      const data = await response.json();

      expect(response.status).toBe(201);
      expect(data).toEqual(mockUser);
    });

    it('should validate input data', async () => {
      const formData = new FormData();
      formData.append('name', ''); // Invalid: empty name
      formData.append('email', 'invalid-email'); // Invalid: bad email format

      const { req } = createMocks({
        method: 'POST',
        headers: { 'content-type': 'multipart/form-data' },
      });

      req.formData = jest.fn().mockResolvedValue(formData);

      const response = await POST(req);

      expect(response.status).toBe(400);
    });
  });
});
\end{lstlisting}

\section{Tests End-to-End}

\subsection{Configuration Playwright}

\begin{lstlisting}[language=TypeScript, caption=Configuration Playwright]
import { defineConfig, devices } from '@playwright/test';

export default defineConfig({
  testDir: './e2e',
  fullyParallel: true,
  forbidOnly: !!process.env.CI,
  retries: process.env.CI ? 2 : 0,
  workers: process.env.CI ? 1 : undefined,
  reporter: 'html',
  use: {
    baseURL: 'http://localhost:3000',
    trace: 'on-first-retry',
    screenshot: 'only-on-failure',
  },
  projects: [
    {
      name: 'chromium',
      use: { ...devices['Desktop Chrome'] },
    },
    {
      name: 'firefox',
      use: { ...devices['Desktop Firefox'] },
    },
    {
      name: 'webkit',
      use: { ...devices['Desktop Safari'] },
    },
    {
      name: 'Mobile Chrome',
      use: { ...devices['Pixel 5'] },
    },
  ],
  webServer: {
    command: 'npm run dev',
    url: 'http://localhost:3000',
    reuseExistingServer: !process.env.CI,
  },
});
\end{lstlisting}

\subsection{Tests E2E des Workflows Utilisateur}

\begin{lstlisting}[language=TypeScript, caption=Tests E2E]
import { test, expect } from '@playwright/test';

test.describe('User Management Workflow', () => {
  test.beforeEach(async ({ page }) => {
    await page.goto('/');
  });

  test('should create a new user', async ({ page }) => {
    // Navigate to user creation page
    await page.click('text=Add User');
    await expect(page).toHaveURL('/users/new');

    // Fill form
    await page.fill('[data-testid=name-input]', 'John Doe');
    await page.fill('[data-testid=email-input]', 'john@example.com');
    
    // Upload profile picture
    await page.setInputFiles('[data-testid=profile-picture-input]', 'test-files/profile.jpg');

    // Submit form
    await page.click('[data-testid=submit-button]');

    // Verify success
    await expect(page).toHaveURL('/users');
    await expect(page.locator('text=John Doe')).toBeVisible();
    await expect(page.locator('text=User created successfully')).toBeVisible();
  });

  test('should edit an existing user', async ({ page }) => {
    // Assume user exists
    await page.goto('/users');
    
    // Click edit button for first user
    await page.click('[data-testid=edit-user-button]');
    
    // Update name
    await page.fill('[data-testid=name-input]', 'John Updated');
    
    // Submit changes
    await page.click('[data-testid=submit-button]');
    
    // Verify update
    await expect(page.locator('text=John Updated')).toBeVisible();
    await expect(page.locator('text=User updated successfully')).toBeVisible();
  });

  test('should delete a user', async ({ page }) => {
    await page.goto('/users');
    
    // Click delete button
    await page.click('[data-testid=delete-user-button]');
    
    // Confirm deletion in modal
    await page.click('text=Confirm Delete');
    
    // Verify deletion
    await expect(page.locator('text=User deleted successfully')).toBeVisible();
  });

  test('should search and filter users', async ({ page }) => {
    await page.goto('/users');
    
    // Use search functionality
    await page.fill('[data-testid=search-input]', 'John');
    
    // Verify filtered results
    await expect(page.locator('[data-testid=user-card]')).toContainText('John');
    
    // Clear search
    await page.fill('[data-testid=search-input]', '');
    
    // Verify all users are shown again
    await expect(page.locator('[data-testid=user-card]')).toHaveCount(3, { timeout: 5000 });
  });
});

test.describe('Performance Tests', () => {
  test('should load homepage within performance budget', async ({ page }) => {
    const startTime = Date.now();
    await page.goto('/');
    const loadTime = Date.now() - startTime;
    
    expect(loadTime).toBeLessThan(3000); // 3 seconds budget
  });

  test('should have good Core Web Vitals', async ({ page }) => {
    await page.goto('/');
    
    // Wait for page to be interactive
    await page.waitForLoadState('networkidle');
    
    // Measure LCP (Largest Contentful Paint)
    const lcp = await page.evaluate(() => {
      return new Promise((resolve) => {
        const observer = new PerformanceObserver((list) => {
          const entries = list.getEntries();
          const lastEntry = entries[entries.length - 1];
          resolve(lastEntry.startTime);
        });
        observer.observe({ entryTypes: ['largest-contentful-paint'] });
      });
    });
    
    expect(lcp).toBeLessThan(2500); // Good LCP is < 2.5s
  });
});
\end{lstlisting}

\section{Tests de Performance}

\subsection{Lighthouse CI Configuration}

\begin{lstlisting}[language=JSON, caption=Configuration Lighthouse CI]
{
  "ci": {
    "collect": {
      "startServerCommand": "npm run build && npm run start",
      "url": ["http://localhost:3000", "http://localhost:3000/users"],
      "numberOfRuns": 3
    },
    "assert": {
      "assertions": {
        "categories:performance": ["error", {"minScore": 0.9}],
        "categories:accessibility": ["error", {"minScore": 0.9}],
        "categories:best-practices": ["error", {"minScore": 0.9}],
        "categories:seo": ["error", {"minScore": 0.9}],
        "first-contentful-paint": ["error", {"maxNumericValue": 2000}],
        "largest-contentful-paint": ["error", {"maxNumericValue": 2500}],
        "cumulative-layout-shift": ["error", {"maxNumericValue": 0.1}]
      }
    },
    "upload": {
      "target": "temporary-public-storage"
    }
  }
}
\end{lstlisting}

\section{Couverture de Code}

\subsection{Métriques de Couverture}

\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|l|c|c|c|c|}
        \hline
        \textbf{Module} & \textbf{Statements} & \textbf{Branches} & \textbf{Functions} & \textbf{Lines} \\
        \hline
        Components & 85.3\% & 78.9\% & 87.2\% & 85.1\% \\
        \hline
        API Routes & 92.1\% & 85.4\% & 94.7\% & 91.8\% \\
        \hline
        Utilities & 89.6\% & 82.3\% & 91.4\% & 89.2\% \\
        \hline
        Services & 87.8\% & 80.1\% & 89.5\% & 87.6\% \\
        \hline
        \textbf{Total} & \textbf{88.7\%} & \textbf{81.7\%} & \textbf{90.7\%} & \textbf{88.4\%} \\
        \hline
    \end{tabularx}
    \caption{Métriques de couverture de code}
    \label{tab:code_coverage}
\end{table}

\subsection{Scripts de Test Automatisés}

\begin{lstlisting}[language=bash, caption=Scripts de test dans package.json]
{
  "scripts": {
    "test": "jest",
    "test:watch": "jest --watch",
    "test:coverage": "jest --coverage",
    "test:e2e": "playwright test",
    "test:e2e:ui": "playwright test --ui",
    "test:lighthouse": "lhci autorun",
    "test:all": "npm run test:coverage && npm run test:e2e && npm run test:lighthouse"
  }
}
\end{lstlisting}

La stratégie de tests complète d'AWS Next Express garantit la fiabilité, la performance et la qualité de l'application à tous les niveaux, depuis les fonctions unitaires jusqu'aux workflows complets des utilisateurs. 