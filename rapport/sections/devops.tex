\chapter{DevOps et Déploiement}

\section{Infrastructure as Code}

\subsection{Architecture de Déploiement}

L'infrastructure d'AWS Next Express est entièrement codifiée et versionnée, permettant une reproductibilité et une traçabilité complètes des déploiements.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/devops_architecture.png}
    \caption{Architecture DevOps complète}
    \label{fig:devops_architecture}
\end{figure}

\subsection{Stratégie Multi-Environnements}

\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|l|X|X|X|}
        \hline
        \textbf{Environnement} & \textbf{Usage} & \textbf{Déploiement} & \textbf{Ressources} \\
        \hline
        Development & Développement local & Manuel, Docker Compose & DynamoDB Local, S3 Mock \\
        \hline
        Staging & Tests d'intégration & Automatique via PR & AWS Resources limitées \\
        \hline
        Production & Application live & Automatique via main branch & AWS Resources complètes \\
        \hline
    \end{tabularx}
    \caption{Stratégie multi-environnements}
    \label{tab:environments}
\end{table}

\section{Containerisation avec Docker}

\subsection{Dockerfile Optimisé}

\begin{lstlisting}[language=Docker, caption=Dockerfile multi-stage]
# Dockerfile
FROM node:18-alpine AS base

# Install dependencies only when needed
FROM base AS deps
RUN apk add --no-cache libc6-compat
WORKDIR /app

# Install dependencies based on the preferred package manager
COPY package.json pnpm-lock.yaml* ./
RUN corepack enable pnpm && pnpm i --frozen-lockfile

# Rebuild the source code only when needed
FROM base AS builder
WORKDIR /app
COPY --from=deps /app/node_modules ./node_modules
COPY . .

# Next.js collects completely anonymous telemetry data about general usage.
ENV NEXT_TELEMETRY_DISABLED 1

RUN corepack enable pnpm && pnpm run build

# Production image, copy all the files and run next
FROM base AS runner
WORKDIR /app

ENV NODE_ENV production
ENV NEXT_TELEMETRY_DISABLED 1

RUN addgroup --system --gid 1001 nodejs
RUN adduser --system --uid 1001 nextjs

COPY --from=builder /app/public ./public

# Set the correct permission for prerender cache
RUN mkdir .next
RUN chown nextjs:nodejs .next

# Automatically leverage output traces to reduce image size
COPY --from=builder --chown=nextjs:nodejs /app/.next/standalone ./
COPY --from=builder --chown=nextjs:nodejs /app/.next/static ./.next/static

USER nextjs

EXPOSE 3000

ENV PORT 3000
ENV HOSTNAME "0.0.0.0"

CMD ["node", "server.js"]
\end{lstlisting}

\subsection{Docker Compose pour le Développement}

\begin{lstlisting}[language=YAML, caption=docker-compose.yml]
version: '3.8'

services:
  frontend:
    build:
      context: .
      dockerfile: Dockerfile
      target: builder
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=development
      - DYNAMODB_ENDPOINT=http://dynamodb-local:8000
      - AWS_ACCESS_KEY_ID=dummy
      - AWS_SECRET_ACCESS_KEY=dummy
      - AWS_REGION=us-east-1
      - DYNAMODB_USERS_TABLE=users
      - AWS_S3_BUCKET_NAME=aws-next-express-dev
    volumes:
      - .:/app
      - /app/node_modules
      - /app/.next
    depends_on:
      - dynamodb-local
    networks:
      - app-network
    restart: unless-stopped
    command: pnpm dev

  dynamodb-local:
    image: amazon/dynamodb-local:latest
    ports:
      - "8000:8000"
    volumes:
      - dynamodb_data:/home/dynamodblocal/data
    networks:
      - app-network
    restart: unless-stopped
    command: ["-jar", "DynamoDBLocal.jar", "-sharedDb", "-dbPath", "/home/dynamodblocal/data"]

  dynamodb-admin:
    image: aaronshaf/dynamodb-admin:latest
    ports:
      - "8001:8001"
    environment:
      - DYNAMO_ENDPOINT=http://dynamodb-local:8000
      - AWS_ACCESS_KEY_ID=dummy
      - AWS_SECRET_ACCESS_KEY=dummy
      - AWS_REGION=us-east-1
    depends_on:
      - dynamodb-local
    networks:
      - app-network
    restart: unless-stopped

volumes:
  dynamodb_data:
    driver: local

networks:
  app-network:
    driver: bridge
\end{lstlisting}

\section{Orchestration Kubernetes}

\subsection{Configuration des Manifests}

\subsubsection{Deployment Frontend}

\begin{lstlisting}[language=YAML, caption=k8s/frontend-deployment.yaml]
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  labels:
    app: aws-next-express
    component: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: aws-next-express
      component: frontend
  template:
    metadata:
      labels:
        app: aws-next-express
        component: frontend
    spec:
      containers:
      - name: frontend
        image: aws-next-express:latest
        ports:
        - containerPort: 3000
          name: http
        env:
        - name: NODE_ENV
          value: "production"
        - name: AWS_REGION
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: region
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: access-key-id
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: secret-access-key
        - name: DYNAMODB_USERS_TABLE
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: dynamodb-users-table
        - name: AWS_S3_BUCKET_NAME
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: s3-bucket-name
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
      imagePullSecrets:
      - name: docker-registry-secret
---
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
  labels:
    app: aws-next-express
    component: frontend
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 3000
    protocol: TCP
    name: http
  selector:
    app: aws-next-express
    component: frontend
\end{lstlisting}

\subsubsection{ConfigMaps et Secrets}

\begin{lstlisting}[language=YAML, caption=k8s/configmap.yaml]
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  dynamodb-users-table: "users"
  s3-bucket-name: "aws-next-express-prod"
  log-level: "info"
  max-file-size: "5242880"
---
apiVersion: v1
kind: Secret
metadata:
  name: aws-credentials
type: Opaque
data:
  region: dXMtZWFzdC0x  # base64 encoded
  access-key-id: YOUR_BASE64_ENCODED_ACCESS_KEY
  secret-access-key: YOUR_BASE64_ENCODED_SECRET_KEY
\end{lstlisting}

\subsubsection{Horizontal Pod Autoscaler}

\begin{lstlisting}[language=YAML, caption=k8s/hpa.yaml]
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: frontend-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: frontend
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
\end{lstlisting}

\section{Pipeline CI/CD}

\subsection{GitHub Actions Workflow}

\begin{lstlisting}[language=YAML, caption=.github/workflows/ci-cd.yml]
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      dynamodb:
        image: amazon/dynamodb-local:latest
        ports:
          - 8000:8000
        options: >-
          --health-cmd "curl -f http://localhost:8000"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'pnpm'

    - name: Install pnpm
      run: corepack enable pnpm

    - name: Install dependencies
      run: pnpm install --frozen-lockfile

    - name: Lint code
      run: pnpm run lint

    - name: Type check
      run: pnpm run type-check

    - name: Run unit tests
      run: pnpm run test:coverage
      env:
        DYNAMODB_ENDPOINT: http://localhost:8000
        AWS_ACCESS_KEY_ID: dummy
        AWS_SECRET_ACCESS_KEY: dummy
        AWS_REGION: us-east-1

    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage/lcov.info
        flags: unittests

    - name: Build application
      run: pnpm run build

    - name: Run E2E tests
      run: pnpm run test:e2e
      env:
        BASE_URL: http://localhost:3000

  security:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'

  build:
    runs-on: ubuntu-latest
    needs: [test, security]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup ArgoCD CLI
      run: |
        curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64
        sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd

    - name: Deploy to staging
      run: |
        argocd login ${{ secrets.ARGOCD_SERVER }} --username ${{ secrets.ARGOCD_USERNAME }} --password ${{ secrets.ARGOCD_PASSWORD }} --insecure
        argocd app sync aws-next-express-staging
        argocd app wait aws-next-express-staging --timeout 300

    - name: Run staging tests
      run: |
        # Run smoke tests against staging environment
        curl -f ${{ secrets.STAGING_URL }}/api/health

    - name: Deploy to production
      if: success()
      run: |
        argocd app sync aws-next-express-production
        argocd app wait aws-next-express-production --timeout 600

    - name: Notify deployment
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        channel: '#deployments'
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}
\end{lstlisting}

\section{ArgoCD GitOps}

\subsection{Application Configuration}

\begin{lstlisting}[language=YAML, caption=argocd/application.yaml]
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: aws-next-express
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default
  source:
    repoURL: https://github.com/nourhb/aws-next-express.git
    targetRevision: main
    path: k8s
  destination:
    server: https://kubernetes.default.svc
    namespace: aws-next-express
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
    - CreateNamespace=true
    - PrunePropagationPolicy=foreground
    - PruneLast=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
  revisionHistoryLimit: 10
---
apiVersion: argoproj.io/v1alpha1
kind: AppProject
metadata:
  name: aws-next-express-project
  namespace: argocd
spec:
  description: AWS Next Express Project
  sourceRepos:
  - 'https://github.com/nourhb/aws-next-express.git'
  destinations:
  - namespace: 'aws-next-express*'
    server: https://kubernetes.default.svc
  clusterResourceWhitelist:
  - group: ''
    kind: Namespace
  - group: 'rbac.authorization.k8s.io'
    kind: ClusterRole
  - group: 'rbac.authorization.k8s.io'
    kind: ClusterRoleBinding
  namespaceResourceWhitelist:
  - group: ''
    kind: Service
  - group: ''
    kind: ConfigMap
  - group: ''
    kind: Secret
  - group: 'apps'
    kind: Deployment
  - group: 'apps'
    kind: ReplicaSet
  - group: 'autoscaling'
    kind: HorizontalPodAutoscaler
\end{lstlisting}

\section{Monitoring et Observabilité}

\subsection{Configuration Prometheus}

\begin{lstlisting}[language=YAML, caption=monitoring/prometheus.yaml]
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
    
    scrape_configs:
    - job_name: 'aws-next-express'
      static_configs:
      - targets: ['frontend-service:80']
      metrics_path: '/api/metrics'
      scrape_interval: 10s
      
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus:latest
        ports:
        - containerPort: 9090
        volumeMounts:
        - name: config
          mountPath: /etc/prometheus
        command:
        - /bin/prometheus
        - --config.file=/etc/prometheus/prometheus.yml
        - --storage.tsdb.path=/prometheus
        - --web.console.libraries=/etc/prometheus/console_libraries
        - --web.console.templates=/etc/prometheus/consoles
        - --web.enable-lifecycle
      volumes:
      - name: config
        configMap:
          name: prometheus-config
\end{lstlisting}

\subsection{Dashboard Grafana}

\begin{lstlisting}[language=JSON, caption=monitoring/grafana-dashboard.json]
{
  "dashboard": {
    "id": null,
    "title": "AWS Next Express Metrics",
    "uid": "aws-next-express",
    "version": 1,
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total{job=\"aws-next-express\"}[5m])",
            "legendFormat": "{{method}} {{status_code}}"
          }
        ],
        "yAxes": [
          {
            "label": "Requests/sec"
          }
        ]
      },
      {
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job=\"aws-next-express\"}[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket{job=\"aws-next-express\"}[5m]))",
            "legendFormat": "50th percentile"
          }
        ],
        "yAxes": [
          {
            "label": "Duration (seconds)"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "singlestat",
        "targets": [
          {
            "expr": "rate(http_requests_total{job=\"aws-next-express\",status_code=~\"5..\"}[5m]) / rate(http_requests_total{job=\"aws-next-express\"}[5m]) * 100",
            "legendFormat": "Error Rate %"
          }
        ]
      }
    ]
  }
}
\end{lstlisting}

\section{Sécurité DevOps}

\subsection{Scan de Sécurité}

\begin{lstlisting}[language=YAML, caption=security/security-scan.yaml]
apiVersion: batch/v1
kind: CronJob
metadata:
  name: security-scan
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: trivy-scanner
            image: aquasec/trivy:latest
            command:
            - /bin/sh
            - -c
            - |
              trivy image --format json --output /tmp/scan-results.json ghcr.io/nourhb/aws-next-express:latest
              # Send results to security team
              curl -X POST -H "Content-Type: application/json" \
                   -d @/tmp/scan-results.json \
                   $SECURITY_WEBHOOK_URL
            env:
            - name: SECURITY_WEBHOOK_URL
              valueFrom:
                secretKeyRef:
                  name: security-config
                  key: webhook-url
          restartPolicy: OnFailure
\end{lstlisting}

Cette architecture DevOps complète assure un déploiement fiable, sécurisé et observable d'AWS Next Express, avec une automatisation totale du pipeline de développement à la production. 